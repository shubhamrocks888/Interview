Perfect! Hereâ€™s a concise, architect-ready EXPLAIN ANALYZE cheat sheet for Postgres (works for MySQL similarly) â€” everything you need to read, interpret, and optimize queries in interviews.

ğŸ—‚ EXPLAIN ANALYZE Cheat Sheet
1ï¸âƒ£ Basics
Command	Purpose
EXPLAIN <query>	Shows plannerâ€™s estimated execution plan without running the query.
EXPLAIN ANALYZE <query>	Runs the query and shows actual execution stats. Useful for detecting real bottlenecks.

Key idea:

Always compare planner estimate vs actual execution to find inefficiencies.

2ï¸âƒ£ Key Output Fields


| Field             | Meaning                                       | Interview Tip                                  |
| ----------------- | --------------------------------------------- | ---------------------------------------------- |
| `Seq Scan`        | Sequential scan (full table scan)             | High rows â†’ candidate for index                |
| `Index Scan`      | Using an index to fetch rows                  | Good for filtering                             |
| `Index Only Scan` | Index has all needed columns â†’ no table fetch | Optimizes reads                                |
| `Nested Loop`     | Loops for joining tables                      | Expensive if large tables â†’ consider Hash Join |
| `Hash Join`       | Hash-based join                               | Good for large datasets                        |
| `Merge Join`      | Merge-based join                              | Efficient if both inputs sorted                |
| `cost=...`        | Plannerâ€™s estimated start & total cost        | Compare with `actual time`                     |
| `rows=`           | Estimated number of rows                      | Compare with actual â†’ detect bad estimates     |
| `actual time=...` | Real execution time for the step              | Focus optimization on high-time steps          |
| `loops=`          | How many times this node ran (nested loops)   | High loops â†’ nested joins are bottleneck       |
| `width=`          | Average row size in bytes                     | Useful for memory planning                     |



3ï¸âƒ£ How to Use It for Optimization

Check estimated vs actual rows

Huge difference â†’ stats outdated or missing index

Identify slow nodes

Look for nodes with highest actual time â†’ target them first

Scan type

Seq Scan on huge tables â†’ consider index or partial index

Nested Loop on large tables â†’ maybe rewrite joins or use Hash Join

Loops

High loops + high rows â†’ query can explode â†’ rewrite or index

Memory / Hash usage

Check temp file creation â†’ increase work_mem or batch queries

Overall

Compare execution time before and after optimization

4ï¸âƒ£ Example Output & Interpretation
Hash Join  (cost=1000.00..5000.00 rows=1000 width=50) (actual time=2.3..5.6 rows=1000 loops=1)
  Hash Cond: (a.id = b.a_id)
  -> Seq Scan on table_a a  (cost=0.00..2000.00 rows=100000 width=20) (actual time=0.1..1.5 rows=100000 loops=1)
  -> Hash  (cost=500.00..500.00 rows=1000 width=30) (actual time=2.0..2.0 rows=1000 loops=1)
        -> Seq Scan on table_b b  (cost=0.00..500.00 rows=1000 width=30) (actual time=0.05..0.07 rows=1000 loops=1)


Takeaways:

Table a: full table scan, 100k rows â†’ candidate for index

Hash Join is efficient here because b is small

Actual time matches estimates â†’ stats are accurate

Loops = 1 â†’ no nested explosion

5ï¸âƒ£ Architect-Level Talking Points

When asked in an interview, say:

â€œI use EXPLAIN ANALYZE to measure actual execution time and rows processed.â€

â€œI compare planner estimates vs actual to see if missing indexes or outdated statistics exist.â€

â€œI focus on the nodes consuming most time or loops to optimize joins, scans, and aggregations.â€

â€œBased on this, I decide between indexing, query rewrite, caching, or batching.â€

Bonus: Mention memory-bound vs CPU-bound nodes â€” shows deeper understanding.

âœ… Quick Reference One-Liner for Interview

â€œEXPLAIN ANALYZE runs the query and shows the execution plan with actual time, rows, loops, and node details, allowing me to pinpoint CPU, memory, and I/O bottlenecks for targeted optimization.â€