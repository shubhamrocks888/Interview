Assume:

~30,000 users

Each user may have:

multiple enrollments

courses

programs

Output: flat report (CSV/XLS needs rows, not objects)

ğŸ§  Step 0: Understand the shape mismatch (VERY IMPORTANT)
Database reality (normalized)
users
enrollments
courses
programs

Report requirement (denormalized)
| User Name | Email | Course | Program | Enrolled At | Status |


ğŸ‘‰ Key insight to say aloud:

â€œReports are always denormalized views of normalized relational data.â€

1ï¸âƒ£ Step 1: Decide the â€œunitâ€ of one report row

Before writing code, you decide:

What does ONE row represent?

In this case:

â¡ï¸ One row = one enrollment

Not:

one user âŒ

one course âŒ

This decision drives everything.

2ï¸âƒ£ Step 2: Identify all required columns

Letâ€™s assume report columns:

user_id
user_name
email
course_name
program_name
enrolled_at
completion_status


Now map each column â†’ source table:

Column	Source
user_name	users
email	users
course_name	courses
program_name	programs
enrolled_at	enrollments
status	enrollments

ğŸ¯ Interviewer phrase:

â€œWe explicitly mapped each report column back to its source to avoid accidental N+1 queries.â€

3ï¸âƒ£ Step 3: Decide data-fetching strategy (CRITICAL)
âŒ Wrong approach (common mistake)
User.all.each do |user|
  user.enrollments.each do |enrollment|
    enrollment.course.name
  end
end


Why it fails:

N+1 queries

Memory explosion

Slow for 30k users

âœ… Correct approach

You start from the report unit â†’ Enrollment

Enrollment
  .joins(:user, :course, :program)
  .where(created_at: from..to)


ğŸ‘‰ Why?

Enrollment = row

Everything else is metadata

4ï¸âƒ£ Step 4: Fetch data in batches (line by line)
Why batching is non-negotiable

30,000 rows Ã— joins = heavy memory

XLS especially is memory-hungry

So:

Enrollment
  .joins(:user, :course, :program)
  .where(created_at: from..to)
  .find_in_batches(batch_size: 1000) do |batch|


ğŸ¯ What to say:

â€œWe never load all rows at once; we process data in batches to control memory.â€

5ï¸âƒ£ Step 5: Select only required columns (VERY IMPORTANT)
âŒ Bad
Enrollment.joins(:user, :course, :program)


Loads:

full user objects

full course objects

unnecessary fields

âœ… Good (explicit select)
Enrollment
  .joins(:user, :course, :program)
  .select(
    "users.id AS user_id",
    "users.name AS user_name",
    "users.email",
    "courses.title AS course_name",
    "programs.title AS program_name",
    "enrollments.created_at AS enrolled_at",
    "enrollments.status"
  )


ğŸ¯ Interviewer language:

â€œWe fetched only the columns required for the report, not full ActiveRecord objects.â€

6ï¸âƒ£ Step 6: Normalize each record (line-by-line)

Now we convert DB rows â†’ flat hashes

Inside batch loop:

batch.each do |record|

Line 1: Create normalized row
row = {}

Line 2: User fields
row[:user_id] = record.user_id
row[:user_name] = record.user_name
row[:email] = record.email

Line 3: Course & program
row[:course_name] = record.course_name
row[:program_name] = record.program_name

Line 4: Enrollment metadata
row[:enrolled_at] = record.enrolled_at.strftime("%Y-%m-%d")
row[:status] = record.status.humanize


Why formatting here?

Reports must be presentation-ready

Avoid formatting during export

ğŸ¯ Key phrase:

â€œNormalization includes formatting dates and enums so the export layer stays dumb.â€

7ï¸âƒ£ Step 7: Handle missing / optional data (REAL-WORLD DETAIL)

Programs might be optional.

row[:program_name] = record.program_name || "N/A"


Why:

CSV/XLS must not break

Stakeholders hate empty cells

8ï¸âƒ£ Step 8: Push normalized rows to exporter

You do NOT store all rows in memory.

exporter.add_row(row)


Exporter might:

write directly to CSV

append to XLS worksheet

stream output

ğŸ¯ Interviewer phrase:

â€œNormalization and exporting are decoupled via a simple interface.â€

9ï¸âƒ£ Step 9: Keep schema consistent

Before loop starts:

HEADERS = [
  :user_id,
  :user_name,
  :email,
  :course_name,
  :program_name,
  :enrolled_at,
  :status
]


Why?

Column order consistency

Easy changes later

ğŸ”Ÿ Step 10: Edge cases you MUST mention
1ï¸âƒ£ Duplicate rows

User enrolled in same course twice

Solution:

DISTINCT ON (user_id, course_id)

2ï¸âƒ£ Soft-deleted users
where(users: { deleted_at: nil })

3ï¸âƒ£ Performance

Indexes on:

enrollments.user_id

enrollments.created_at

1ï¸âƒ£1ï¸âƒ£ Final summary answer (verbatim-ready)

â€œWe defined the report row as one enrollment and started querying from enrollments instead of users. We explicitly selected only required columns using joins, processed data in batches of 1,000 to control memory, and normalized each record into a flat hash by formatting dates, enums, and handling optional values. This normalized layer was independent of the export format and streamed directly to the exporter to avoid loading all data in memory.â€