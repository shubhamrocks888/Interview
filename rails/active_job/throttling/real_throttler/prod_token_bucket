class RedisTokenBucket
  def initialize(capacity:, refill_rate:)
    @capacity = capacity
    @refill_rate = refill_rate
  end

  def allow?(key)
    now = Time.now.to_f

    data = Rails.cache.read(bucket_key(key)) || {
      tokens: @capacity,
      last_refill: now
    }

    tokens = data[:tokens]
    last_refill = data[:last_refill]

    # refill
    elapsed = now - last_refill
    tokens = [@capacity, tokens + elapsed * @refill_rate].min

    if tokens >= 1
      tokens -= 1
      Rails.cache.write(
        bucket_key(key),
        { tokens: tokens, last_refill: now },
        expires_in: ttl
      )
      true
    else
      Rails.cache.write(
        bucket_key(key),
        { tokens: tokens, last_refill: now },
        expires_in: ttl
      )
      false
    end
  end

  private

  def bucket_key(key)
    "token_bucket:#{key}"
  end

  def ttl
    (@capacity / @refill_rate).ceil
  end
end



##########

8ï¸âƒ£ Important interview note âš ï¸

This Redis version is logically correct but still has:

race conditions under concurrency

True production systems use:

Redis Lua scripts

Atomic math in Redis

Or specialized gateways (Envoy, Nginx)

If you mention this â†’ you sound senior ğŸ‘Œ

9ï¸âƒ£ Interview-ready explanation ğŸ¯

You can say:

â€œToken bucket allows short bursts by consuming pre-filled tokens, then enforces an average rate using time-based refill. Itâ€™s closer to real throttling than fixed-window rate limiting.â€

That answer is ğŸ”¥.

ğŸ”Ÿ Final takeaway

âœ”ï¸ Rate limiter â†’ counts requests

âœ”ï¸ Token bucket â†’ controls flow

âœ”ï¸ Your intuition about slow â†’ reject was correct

âœ”ï¸ This is real throttling


