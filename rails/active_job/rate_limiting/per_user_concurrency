def perform(user_id)
	MAX_CONCURRENCY = 2
	RETRY = 5.seconds

	def perfom(user_id)
		key = "welcome:email:concurrency:user-id:#{user_id}"

		count = Rails.cache.increment(key, 1, expires_in: 1.minute)

		if count > MAX_CONCURRENCY
			Rails.cache.decrement(key)
			self.class.perform_in(RETRY, user_id)
			return
		end

		begin
			UserMailer.welcome_email(user_id).deliver_now
		ensure
			Rails.cache.decrement(key)
		end
	end
end


##########


ğŸš¨ Whatâ€™s wrong with this code (real prod issues)
1ï¸âƒ£ Silent job loss (same bug again)
return if current > 2


Job is acknowledged as success

No retry

No reschedule

Job is lost forever

âŒ Unacceptable in production

2ï¸âƒ£ Over-increment bug

Even rejected jobs do:

Rails.cache.increment(key)


Then return âŒ
So concurrency counter keeps increasing until TTL expires.

Result:

You block the user for 5 minutes

Even though nothing is running

3ï¸âƒ£ Crash window = leaked slot
ensure
  Rails.cache.decrement(key)


If:

process is SIGKILLed

machine dies

Redis hiccup

â¡ decrement never happens
â¡ user stuck until TTL

TTL helps, but still sloppy.

4ï¸âƒ£ Mailer is blocking
deliver_now


Occupies Sidekiq thread

Makes concurrency tracking inaccurate

Slows throughput

5ï¸âƒ£ No fairness / starvation protection

One user can still:

enqueue infinite jobs

starve themselves forever

ğŸ§  Key insight (this is senior-level)

Concurrency limit â‰  admission control

You must:

Acquire slot

If failed â†’ retry later

If acquired â†’ always release

Never leak or inflate counters

âœ… Production-ready OSS concurrency limiter
Pattern: Try â†’ Delay â†’ Execute â†’ Release
MAX_CONCURRENCY = 2
RETRY_DELAY    = 5.seconds

def perform(user_id)
  key = "welcome_email_concurrency:user:#{user_id}"

  current = Rails.cache.increment(key, 1, expires_in: 10.minutes)

  if current > MAX_CONCURRENCY
    Rails.cache.decrement(key)
    self.class.perform_in(RETRY_DELAY, user_id)
    return
  end

  begin
    UserMailer.welcome_email(user_id).deliver_now
  ensure
    Rails.cache.decrement(key)
  end
end

Why this is better

âœ… No silent drops
âœ… No counter inflation
âœ… Slot always released
âœ… Crash-safe via TTL
âœ… Backpressure instead of failure

ğŸ’ Still not perfect (honesty)

Even this:

Can still starve under extreme load

Needs jitter to avoid thundering herd

Needs max retry protection

This is why Sidekiq Enterprise exists.

ğŸ’ Enterprise version (actual correct solution)
sidekiq_options concurrency: { limit: 2, key: ->(args) { args.first } }


Atomic

Fair

Crash-safe

Zero custom logic

ğŸ§  Interview-ready explanation (ğŸ”¥)

Naive cache-based concurrency limits either leak slots or drop jobs.
A production-safe design must release counters on rejection, retry with backoff, and protect against crashes.

TL;DR

| Version                  | Prod ready |
| ------------------------ | ---------- |
| Naive increment/return   | âŒ          |
| With decrement on reject | âš ï¸         |
| With delay + ensure      | âœ…          |
| Sidekiq Enterprise       | ğŸ’         |
