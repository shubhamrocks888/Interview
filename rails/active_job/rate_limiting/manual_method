âœ… Correct Rails.cache version (atomic + TTL)
def perform(user_id)
  key = "welcome_email_rate"

  count = Rails.cache.increment(key, 1, expires_in: 60)

  return if count > 10

  UserMailer.welcome_email(user_id).deliver_now
end


NOTE:  

1ï¸âƒ£ Hard rate limit (DROP excess)

â€œAllow only 10 per minute, drop the restâ€

â¡ Your code is correct


Why this works

increment is atomic (when cache store is Redis or Memcached)

expires_in ensures a 60-second window

No race conditions

Much cleaner than raw Redis code

âš ï¸ VERY IMPORTANT (interview trap ğŸš¨)
This ONLY works if your cache store is:

âœ… :redis_cache_store
âœ… :mem_cache_store

âŒ NOT :memory_store
âŒ NOT :file_store

Because:

increment is NOT atomic in memory/file stores

Multiple Sidekiq processes = broken rate limiting

âœ… Correct cache config (production-safe)
# config/environments/production.rb
config.cache_store = :redis_cache_store, {
  url: ENV['REDIS_URL'],
  expires_in: 1.hour
}

ğŸ¯ Per-user rate limiting (real-world)

Your example limits globally.
Usually you want per user ğŸ‘‡

def perform(user_id)
  key = "welcome_email_rate:user:#{user_id}"

  count = Rails.cache.increment(key, 1, expires_in: 60)

  return if count > 10

  UserMailer.welcome_email(user_id).deliver_now
end

âš ï¸ One more improvement (mail safety)

If Sidekiq retries, email may send twice.
To prevent that, combine with uniqueness:

sidekiq_options unique: :until_executed


(Requires sidekiq-unique-jobs)

ğŸ§  Interview-ready explanation (short & crisp)

We use Rails.cache.increment with Redis cache store to enforce atomic per-user rate limiting.
TTL defines the sliding window and avoids race conditions across Sidekiq workers.

â“When NOT to use Rails.cache

âŒ You need strict ordering / fairness
âŒ You need distributed throttling across services
âŒ You need exact rate windows

Then â†’ Redis Lua / Sidekiq Enterprise.