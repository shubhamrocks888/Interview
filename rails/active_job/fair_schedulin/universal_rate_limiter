class RateLimiter
	LIMIT = 5
	WINDOW = 1.minute

	def self.allowed?(user_id,action)
		cache_key = "rate:#{action}:user:#{user-id}"

		current = Rails.cache.increment(
		cache_key,
		1,
		expires_in: WINDOW,
		initial: 0)

		count <= LIMIT
	end
end

#################

1ï¸âƒ£ Real-time RATE LIMITING (HTTP requests)
Requirement

User can hit endpoint 10 times per minute

Redis-based implementation (Rails.cache)
class RateLimiter
  LIMIT = 10
  WINDOW = 1.minute

  def self.allowed?(user_id, action)
    key = "rate:#{action}:user:#{user_id}"

    count = Rails.cache.increment(
      key,
      1,
      expires_in: WINDOW,
      initial: 0
    )

    count <= LIMIT
  end
end

Controller usage
before_action :check_rate_limit

def check_rate_limit
  unless RateLimiter.allowed?(current_user.id, action_name)
    render json: { error: "Too many requests" }, status: 429
  end
end

ðŸ§  What this gives you

âœ” Per-user
âœ” Per-action
âœ” Works across servers
âœ” Auto-reset via TTL

ðŸ§± 2ï¸âƒ£ Real-time CONCURRENCY LIMITING (HTTP requests)
Requirement

User can only have 1 active report generation at a time

Implementation
class ConcurrencyLimiter
  TTL = 10.minutes

  def self.acquire(user_id, action)
    key = "lock:#{action}:user:#{user_id}"

    acquired = Rails.cache.write(
      key,
      true,
      expires_in: TTL,
      unless_exist: true
    )

    acquired
  end

  def self.release(user_id, action)
    Rails.cache.delete("lock:#{action}:user:#{user_id}")
  end
end

Controller usage
def generate_report
  unless ConcurrencyLimiter.acquire(current_user.id, :report)
    render json: { error: "Report already in progress" }, status: 409
    return
  end

  begin
    enqueue_report_job
    render json: { status: "started" }
  ensure
    # Release immediately if only protecting enqueue,
    # or release later via Sidekiq callback
  end
end

ðŸ§  Why TTL again?

Same reason as Sidekiq:

App crash

Request aborted

Server restart

TTL guarantees recovery.

ðŸ§± 3ï¸âƒ£ Combine RATE + CONCURRENCY (real world)
def generate_report
  unless RateLimiter.allowed?(current_user.id, :report)
    render status: 429
    return
  end

  unless ConcurrencyLimiter.acquire(current_user.id, :report)
    render status: 409
    return
  end

  enqueue_report_job
end

ðŸ”¥ Production-grade improvement (VERY STRONG)
Release concurrency from Sidekiq

Acquire lock in controller

Release lock when background job finishes

This ties real-time API and background execution together.

ðŸ§  Interview-ready summary (say this)

â€œFor real-time user actions, we implement rate and concurrency limiting using Redis.
Redis counters with TTL handle rate limits, and Redis locks with TTL handle concurrency.
This works across app servers and provides crash safety, similar to Sidekiq Enterprise limiters.â€

ðŸ† Bonus points (if interviewer is senior)

Mention:

Rack middleware

NGINX / Envoy rate limiting

Sliding window / token bucket

API Gateway limits

Example line:

â€œAt scale, we often push rate limiting to API gateways or edge proxies.â€

ðŸ Final mental model
HTTP request
   â†“
Rate limiter (Redis + TTL)
   â†“
Concurrency limiter (Redis + TTL)
   â†“
Business logic
