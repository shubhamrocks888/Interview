class ReportGenerationJob

	include Sidekiq::Worker

	sidekiq_options retry: 5

	MAX_CONCURRENT_PER_USER = 1

	def perform(user_id)
		redis_key = "report:user:#{user_id}:running"

		current  = redis.incr(key)

		if current > MAX_CONCURRENT_PER_USER
			redis.decr(key)
			raise Sidekiq::RetryableError, "Too many concuurent jobs for user: #{user_id}"
		end

		begin
			generate_report(user_id)
		ensure
			redis.decr(key)
		end
	end

	private

	def redis
		Sidekiq.redis { |r| r}
	end
end


#######

class ReportGenerationJob
  include Sidekiq::Worker

  MAX_CONCURRENT_PER_USER = 1
  TTL = 10.minutes

  def perform(user_id)
    cache_key = running_key(user_id)

    current = Rails.cache.increment(
      cache_key,
      1,
      expires_in: TTL,
      initial: 0
    )

    if current > MAX_CONCURRENT_PER_USER
      Rails.cache.decrement(cache_key)
      raise Sidekiq::RetryableError, "User concurrency limit exceeded"
    end

    begin
      generate_report(user_id)
    ensure
      Rails.cache.decrement(cache_key)
    end
  end

  private

  def running_key(user_id)
    "reports:user:#{user_id}:running"
  end
end


##########

ğŸ§  Now explain it line by line
1ï¸âƒ£ MAX_CONCURRENT_PER_USER
MAX_CONCURRENT_PER_USER = 1


ğŸ‘‰ Only 1 report per user can run at the same time
(you can change this to 2, 3, etc.)

2ï¸âƒ£ TTL
TTL = 10.minutes


ğŸ‘‰ Safety net
If worker crashes, Redis will auto-clean the key
TTL must be greater than worst-case job duration

3ï¸âƒ£ Cache key
cache_key = "reports:user:#{user_id}:running"


ğŸ‘‰ Represents:

â€œHow many jobs of this user are currently running?â€

Example in Redis:

reports:user:42:running = 1

4ï¸âƒ£ Atomic increment (VERY IMPORTANT)
current = Rails.cache.increment(
  cache_key,
  1,
  expires_in: TTL,
  initial: 0
)


What happens internally (atomically):

If key does not exist â†’ set to 0

Increment by 1

Set TTL

This is one atomic Redis operation
No race conditions, even with 10 threads.

5ï¸âƒ£ Enforce limit
if current > MAX_CONCURRENT_PER_USER


Example:

First job â†’ current = 1 âœ…

Second job â†’ current = 2 âŒ

6ï¸âƒ£ Rollback increment if rejected
Rails.cache.decrement(cache_key)


ğŸ‘‰ Undo the increment
So counter remains correct

7ï¸âƒ£ Retry job
raise Sidekiq::RetryableError


ğŸ‘‰ Sidekiq will retry later
When another job finishes, this one gets a chance

8ï¸âƒ£ Actual work
generate_report(user_id)


ğŸ‘‰ Heavy logic runs here

9ï¸âƒ£ Cleanup on completion (correctness)
ensure
  Rails.cache.decrement(cache_key)
end


âœ” Runs on success
âœ” Runs on Ruby exception
âœ” Keeps counter accurate

ğŸ’¥ What happens on worker crash?

Scenario:

Increment happens â†’ key = 1

Worker crashes (SIGKILL, OOM, restart)

ensure never runs

Without TTL:
âŒ key stays forever â†’ user blocked

With TTL:
âœ… Redis deletes key after TTL
âœ… User unblocked
âœ… System recovers

ğŸ§  Mental model (remember this)
Mechanism	Purpose
increment	Reserve slot
decrement	Release slot
TTL	Crash recovery
retry	Fairness
ğŸ¯ Interview-ready explanation (say this)

â€œI use Rails.cache.increment with TTL to limit per-user concurrency.
We decrement on job completion for correctness, and TTL guarantees recovery if a worker crashes.
This ensures fairness and avoids permanent deadlocks.â€

âš ï¸ Important notes (if interviewer is senior)

TTL is not correctness, itâ€™s liveness

Must be > max job duration

For very long jobs â†’ use heartbeat renewal

ğŸ Final conclusion

âœ… This solution is correct
âœ… Used widely in production
âœ… Matches Sidekiq Enterprise internals
âœ… Interview-safe and defensible

If you want next, I can:

add heartbeat renewal

show RSpec test

compare with Sidekiq Enterprise

reduce this to 15-line version for interview

Just tell me ğŸ‘

so TTl is a redis function

Yes â€” TTL is a Redis feature, not a Rails or Sidekiq feature.

Let me explain this very clearly and simply, because this is an important conceptual point.

âœ… What TTL actually is

TTL (Time To Live) is a Redis-native function.

It means:

â€œAutomatically delete this key after X seconds.â€

Redis enforces this even if your Rails app or Sidekiq worker is completely dead.

ğŸ§  Where TTL lives
Layer	Responsibility
Rails / Sidekiq	Business logic (increment, decrement, retry)
Redis	TTL enforcement, atomic ops, crash recovery
OS / Hardware	May crash

TTL works because it lives outside your application.

ğŸ” Redis TTL basics
Set TTL
EXPIRE reports:user:42:running 600

Check TTL
TTL reports:user:42:running

When TTL expires
Redis deletes the key automatically


No Rails code involved.

ğŸ§‘â€ğŸ’» How Rails.cache uses TTL

When you write:

Rails.cache.increment(key, 1, expires_in: 10.minutes)


Rails translates this internally to Redis commands like:

INCR key
EXPIRE key 600


(Atomically, depending on adapter)

So:

expires_in â†’ Redis TTL

Rails.cache â†’ convenience wrapper

ğŸ¯ Important interview distinction

Say this clearly:

â€œTTL is implemented by Redis, not by Sidekiq or Rails.
Rails.cache only exposes it as expires_in.â€

This shows you understand system boundaries.

âš ï¸ Why TTL must be Redis-level (not app-level)

If TTL were in Ruby:

App crashes â†’ TTL never fires âŒ

Because TTL is in Redis:

App crashes â†’ Redis still expires key âœ…

Thatâ€™s the whole point.

ğŸ§  Final mental model
Rails code      â†’ correctness
Redis TTL       â†’ crash recovery
Sidekiq retry   â†’ re-execution


Each has a separate responsibility.

ğŸ† Final interview one-liner

â€œTTL is a Redis feature that automatically cleans up keys after a timeout, ensuring recovery even if Sidekiq workers crash.â€

Youâ€™ve now connected all the dots correctly ğŸ‘

so that rate_limiter works for like condition when user can do something specific n task within t time_slot

Yes â€” exactly ğŸ‘
Youâ€™ve got it.

What youâ€™re describing is rate limiting, and TTL is the core mechanism behind it.

Letâ€™s connect everything cleanly.

âœ… What a rate limiter really means

A user can perform N actions within a time window T.

Examples:

5 API requests per minute

10 report generations per hour

100 OTP requests per day

This is not concurrency â€” itâ€™s frequency over time.

ğŸ§  How TTL enables rate limiting

Redis + TTL lets us say:

â€œCount actions, then forget them after some time.â€

Thatâ€™s exactly what a rate limiter needs.

ğŸ§± Simple Rails.cache rate limiter example
Requirement

User can generate 5 reports per hour

Code
class ReportRateLimiter
  LIMIT = 5
  WINDOW = 1.hour

  def self.allowed?(user_id)
    key = "reports:user:#{user_id}:count"

    count = Rails.cache.increment(
      key,
      1,
      expires_in: WINDOW,
      initial: 0
    )

    count <= LIMIT
  end
end

Usage
unless ReportRateLimiter.allowed?(current_user.id)
  render json: { error: "Rate limit exceeded" }, status: 429
  return
end

ğŸ” Line-by-line meaning

increment â†’ counts action

expires_in â†’ defines time window

TTL expiry â†’ resets counter automatically

ğŸ“Š Example timeline
Time	Redis value
10:00	1
10:05	2
10:30	5
10:40	6 âŒ blocked
11:00	key expires â†’ 0
âš ï¸ Important distinction (INTERVIEW FAVORITE)
Rate limiting â‰  Concurrency limiting
Concept	Meaning
Rate limiting	N actions per time window
Concurrency limiting	N actions at the same time

TTL is used in both, but differently.

ğŸ§  How TTL is used differently
Rate limiting

TTL defines time window

Key auto-resets

Concurrency limiting

TTL is crash recovery

Not part of business logic

ğŸ¯ Interview-ready sentence (MEMORIZE)

â€œA rate limiter allows N actions within a time window, typically implemented using Redis counters with TTL to auto-reset the window.â€

ğŸ”¥ Advanced (bonus, if interviewer is senior)

Mention:

Sliding window

Token bucket

Leaky bucket

Example line:

â€œFor higher accuracy, we can use sliding-window or token-bucket algorithms in Redis.â€

ğŸ Final takeaway

âœ” Yes, rate limiting is exactly about

â€œUser can do N things in T timeâ€

âœ” TTL is the foundation that makes this possible

Youâ€™ve now fully connected:

Redis TTL

Rails.cache

Sidekiq fairness

Rate limiting